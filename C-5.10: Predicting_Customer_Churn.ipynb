{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/usshaa/SMBDA/blob/main/C-5.10%3A%20Predicting_Customer_Churn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "434714d2-17d3-48bb-a1b2-d30ba09e58d6",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "G6oIASQeeEF0"
      },
      "source": [
        "### Building Machine Learning Pipelines in PySpark\n",
        "\n",
        "Machine learning pipelines in PySpark streamline the process of building and deploying machine learning models by chaining multiple stages, including data preprocessing, feature engineering, model training, and evaluation, into a cohesive workflow. Here's a detailed guide on building a machine learning pipeline using PySpark.\n",
        "\n",
        "### Example: Predicting Customer Churn\n",
        "\n",
        "We'll build a machine learning pipeline for a customer churn prediction task using a hypothetical dataset with columns: `User Id`, `First Name`, `Last Name`, `Sex`, `Email`, `Phone`, `Date of birth`, `Job Title`, and a binary label `Churn`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "2119f08c-6260-47ec-b41a-82d84a718b39",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "jr4sjOMweEF8",
        "outputId": "8a9e97ad-a76d-4351-99b2-b20395cd7f05"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
              "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)\n",
              "File \u001b[0;32m<command-4125139553725403>:7\u001b[0m\n",
              "\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pipeline\n",
              "\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BinaryClassificationEvaluator\n",
              "\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfaker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Faker\n",
              "\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m choice\n",
              "\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
              "\n",
              "File \u001b[0;32m/databricks/python_shell/dbruntime/PythonPackageImportsInstrumentation/__init__.py:171\u001b[0m, in \u001b[0;36m_create_import_patch.<locals>.import_patch\u001b[0;34m(name, globals, locals, fromlist, level)\u001b[0m\n",
              "\u001b[1;32m    166\u001b[0m thread_local\u001b[38;5;241m.\u001b[39m_nest_level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
              "\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
              "\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# Import the desired module. If you’re seeing this while debugging a failed import,\u001b[39;00m\n",
              "\u001b[1;32m    170\u001b[0m     \u001b[38;5;66;03m# look at preceding stack frames for relevant error information.\u001b[39;00m\n",
              "\u001b[0;32m--> 171\u001b[0m     original_result \u001b[38;5;241m=\u001b[39m \u001b[43mpython_builtin_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlocals\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfromlist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
              "\u001b[1;32m    173\u001b[0m     is_root_import \u001b[38;5;241m=\u001b[39m thread_local\u001b[38;5;241m.\u001b[39m_nest_level \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
              "\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# `level` represents the number of leading dots in a relative import statement.\u001b[39;00m\n",
              "\u001b[1;32m    175\u001b[0m     \u001b[38;5;66;03m# If it's zero, then this is an absolute import.\u001b[39;00m\n",
              "\n",
              "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'faker'"
            ]
          },
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "arguments": {},
              "data": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)\nFile \u001b[0;32m<command-4125139553725403>:7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pipeline\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BinaryClassificationEvaluator\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfaker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Faker\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m choice\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\nFile \u001b[0;32m/databricks/python_shell/dbruntime/PythonPackageImportsInstrumentation/__init__.py:171\u001b[0m, in \u001b[0;36m_create_import_patch.<locals>.import_patch\u001b[0;34m(name, globals, locals, fromlist, level)\u001b[0m\n\u001b[1;32m    166\u001b[0m thread_local\u001b[38;5;241m.\u001b[39m_nest_level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# Import the desired module. If you’re seeing this while debugging a failed import,\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;66;03m# look at preceding stack frames for relevant error information.\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m     original_result \u001b[38;5;241m=\u001b[39m \u001b[43mpython_builtin_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlocals\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfromlist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m     is_root_import \u001b[38;5;241m=\u001b[39m thread_local\u001b[38;5;241m.\u001b[39m_nest_level \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# `level` represents the number of leading dots in a relative import statement.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;66;03m# If it's zero, then this is an absolute import.\u001b[39;00m\n\n\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'faker'",
              "errorSummary": "<span class='ansi-red-fg'>ModuleNotFoundError</span>: No module named 'faker'",
              "errorTraceType": "ansi",
              "metadata": {},
              "type": "ipynbError"
            }
          }
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, datediff, current_date\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from faker import Faker\n",
        "from random import choice\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "99e20568-a0c4-4e4c-84f1-eaa05e2be3fc",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "mgHv3GJkeEGA"
      },
      "outputs": [],
      "source": [
        "# Step 1: Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"ML Pipeline Example\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "014c68d5-cf75-448a-8e4b-6bade9785a7a",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "xngRZcxGeEGB"
      },
      "outputs": [],
      "source": [
        "# Step 2: Load Data\n",
        "data = spark.read.csv(\"/FileStore/tables/customer_data.csv\", header=True, inferSchema=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "bccbd0ac-d39c-49d5-8de7-69043f4cad26",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "PvN2h16neEGD"
      },
      "outputs": [],
      "source": [
        "# Step 3: Data Preprocessing\n",
        "indexer_sex = StringIndexer(inputCol=\"Sex\", outputCol=\"SexIndex\")\n",
        "indexer_job = StringIndexer(inputCol=\"Job Title\", outputCol=\"JobTitleIndex\", handleInvalid=\"keep\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "3bd67097-9e8d-4750-9dfb-87ff6fc7418a",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "mwoJXguveEGE"
      },
      "outputs": [],
      "source": [
        "# Calculate Age from Date of Birth\n",
        "data = data.withColumn(\"Age\", (datediff(current_date(), col(\"Date of birth\")) / 365).cast(\"int\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "00051901-ec8e-4ff7-bf3c-2cd24194dd1f",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "BXJUjH8EeEGF"
      },
      "outputs": [],
      "source": [
        "# Select relevant features for the model\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[\"Age\", \"SexIndex\", \"JobTitleIndex\"],\n",
        "    outputCol=\"features\"\n",
        ")\n",
        "\n",
        "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "03a0c21d-3c5f-4b84-bc12-5bd44cf4b937",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "af6myIBdeEGH"
      },
      "outputs": [],
      "source": [
        "# Step 4: Build Machine Learning Pipeline\n",
        "lr = LogisticRegression(featuresCol=\"scaledFeatures\", labelCol=\"Churn\")\n",
        "pipeline = Pipeline(stages=[indexer_sex, indexer_job, assembler, scaler, lr])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "84a3a7e5-bccb-40a5-b7b1-9643c53e65f8",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "99mWtR9leEGH"
      },
      "outputs": [],
      "source": [
        "# Step 5: Train and Evaluate the Model\n",
        "train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "1af973ff-9e71-468b-a977-eda16de64cb6",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "dxNgBMSveEGJ"
      },
      "outputs": [],
      "source": [
        "model = pipeline.fit(train_data)\n",
        "predictions = model.transform(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "dbb17c9d-4ed1-4320-8809-ad07d7952913",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "Keu9w-KDeEGJ"
      },
      "outputs": [],
      "source": [
        "evaluator = BinaryClassificationEvaluator(labelCol=\"Churn\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "c03e7627-150f-4b46-b1ff-85c7a612c1d5",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "raX1hXR6eEGK",
        "outputId": "5b28cc5a-8c9e-4983-ca05-826fe75b27c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.54\n"
          ]
        }
      ],
      "source": [
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "6410f118-eaf9-4a67-8c5a-42d40013b465",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "2tbDqNG1eEGL",
        "outputId": "da7be5de-481b-4bb9-9b92-ff5042f3de3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.58\nRecall: 0.56\nF1-score: 0.53\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# Create a MulticlassClassificationEvaluator\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"Churn\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
        "precision = evaluator.evaluate(predictions)\n",
        "\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"Churn\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
        "recall = evaluator.evaluate(predictions)\n",
        "\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"Churn\", predictionCol=\"prediction\", metricName=\"f1\")\n",
        "f1_score = evaluator.evaluate(predictions)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-score: {f1_score:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "fd7e7182-c33e-4eae-bc9b-4b6799e3dfb0",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "8zsca9oQeEGL",
        "outputId": "d51b416f-f8cb-4537-be95-990a6b0c4e38"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
            "text/plain": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>prediction</th>\n",
              "      <th>0.0</th>\n",
              "      <th>1.0</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Churn</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0.0</th>\n",
              "      <td>26</td>\n",
              "      <td>56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.0</th>\n",
              "      <td>16</td>\n",
              "      <td>64</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "addedWidgets": {},
              "arguments": {},
              "data": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>prediction</th>\n      <th>0.0</th>\n      <th>1.0</th>\n    </tr>\n    <tr>\n      <th>Churn</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0.0</th>\n      <td>26</td>\n      <td>56</td>\n    </tr>\n    <tr>\n      <th>1.0</th>\n      <td>16</td>\n      <td>64</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
              "datasetInfos": [],
              "metadata": {},
              "removedWidgets": [],
              "textData": null,
              "type": "htmlSandbox"
            }
          }
        }
      ],
      "source": [
        "from pyspark.sql import functions as F\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming 'predictions' is your DataFrame containing predictions\n",
        "predictions = predictions.withColumn(\"prediction\", F.col(\"prediction\").cast(\"double\"))\n",
        "predictions = predictions.withColumn(\"Churn\", F.col(\"Churn\").cast(\"double\"))\n",
        "\n",
        "# Group by true and predicted labels and count occurrences\n",
        "conf_matrix = predictions.groupBy(\"Churn\", \"prediction\").count()\n",
        "\n",
        "# Convert to Pandas DataFrame for easier manipulation\n",
        "conf_matrix_pd = conf_matrix.toPandas()\n",
        "\n",
        "# Pivot the DataFrame to get confusion matrix\n",
        "conf_matrix_pd = conf_matrix_pd.pivot(index='Churn', columns='prediction', values='count').fillna(0)\n",
        "\n",
        "# Display confusion matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "conf_matrix_pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "14e0ae1e-6333-4249-ae03-e35308e79ba6",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "mx7VXBF2eEGM"
      },
      "outputs": [],
      "source": [
        "# Step 6: Save and Load Model (Optional)\n",
        "model.save(\"pipeline_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "5fc36612-d4b4-4b47-9cdc-872c6803e220",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "icvv1IpgeEGM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "computePreferences": null,
      "dashboards": [],
      "environmentMetadata": null,
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 4
      },
      "notebookName": "10-Predicting Customer Churn",
      "widgets": {}
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}