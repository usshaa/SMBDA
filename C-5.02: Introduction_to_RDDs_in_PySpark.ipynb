{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/usshaa/SMBDA/blob/main/C-5.1%3A%20Introduction_to_RDDs_in_PySpark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "c9023924-a348-48b4-87b7-fea2e3253559",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "lkNhvGldC-il"
      },
      "source": [
        "## Introduction to RDDs in PySpark\n",
        "\n",
        "Resilient Distributed Datasets (RDDs) are the fundamental data structure of Apache Spark. RDDs are fault-tolerant collections of elements that can be operated on in parallel. This section will introduce basic RDD operations, including creation, transformations, and actions.\n",
        "\n",
        "### 1. Creating RDDs\n",
        "\n",
        "#### From a Python List\n",
        "\n",
        "You can create an RDD from a Python list using the `parallelize` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "900fe193-2601-4275-a77b-dfdeacd6291d",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "TalwuP7eC-in"
      },
      "outputs": [],
      "source": [
        "# Set the PySpark environment variables\n",
        "import os\n",
        "os.environ['SPARK_HOME'] = 'spark-3.4.3-bin-hadoop3'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "dc302cec-5fd5-4d66-a9ac-fbda3bd612e6",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "dTdXe9m2C-io"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "ff1f1880-0c49-4bc4-9978-7ea0e1be2892",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "QKMTDW9VC-io"
      },
      "outputs": [],
      "source": [
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder.appName(\"RDD_Basics\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "12bf8504-b467-4a06-81d6-342e2dbce91c",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "2jXpYIJ8C-io"
      },
      "outputs": [],
      "source": [
        "# Get the SparkContext from the Spark session\n",
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "f61f5521-36ee-4136-bfe5-97acb908104d",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "658KP2rKC-io"
      },
      "outputs": [],
      "source": [
        "# Create an RDD from a Python list\n",
        "numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "numbers_rdd = sc.parallelize(numbers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "83c31dde-d9c8-4162-8ff5-20c88b29e2e0",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "V7HivWs9C-io",
        "outputId": "9e4455e2-bd8a-43bc-f6df-0e7d21c62f27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Out[23]: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
          ]
        }
      ],
      "source": [
        "# Collect the RDD to view its contents\n",
        "numbers_rdd.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "c3c8690e-e695-4a5c-afbb-afb011f1bf86",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "tb-ErGeNC-ip"
      },
      "source": [
        "### 2. Basic Transformations\n",
        "\n",
        "Transformations create a new RDD from an existing one. They are \"lazy,\" meaning they don't execute until an action is called.\n",
        "\n",
        "#### Map\n",
        "\n",
        "The `map` transformation applies a function to each element in the RDD and returns a new RDD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "7755b07e-2b3f-4e75-8411-4c9397d84254",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "YxVEI9HLC-ip",
        "outputId": "3e3902fb-905d-43ae-8283-c989987da634"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Out[24]: PythonRDD[31] at RDD at PythonRDD.scala:58"
          ]
        }
      ],
      "source": [
        "# Square each number in the RDD\n",
        "squared_numbers_rdd = numbers_rdd.map(lambda x: x ** 2)\n",
        "squared_numbers_rdd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "8dda7d86-16fa-44c6-a353-874e1eb371bd",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "YSrAqMfuC-ip",
        "outputId": "51ca2fe2-4a11-4b42-e6ce-7a101c586108"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Out[25]: [1, 4, 9, 16, 25, 36, 49, 64, 81, 100]"
          ]
        }
      ],
      "source": [
        "# Collect the RDD to view its contents\n",
        "squared_numbers_rdd.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "831f9e77-bee9-49f3-b3e9-a7273224ab09",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "TRvkSNYzC-ip"
      },
      "source": [
        "#### Filter\n",
        "\n",
        "The `filter` transformation creates a new RDD by selecting elements that satisfy a predicate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "fba9a766-1175-4ea3-8cdc-d3c1249abad4",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "xN4q3z07C-ip",
        "outputId": "65761fc4-e81d-4ebc-b8a6-444adffc9b2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Out[26]: PythonRDD[32] at RDD at PythonRDD.scala:58"
          ]
        }
      ],
      "source": [
        "# Keep only even numbers from the squared RDD\n",
        "even_squared_numbers_rdd = squared_numbers_rdd.filter(lambda x: x % 2 == 0)\n",
        "even_squared_numbers_rdd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "f0cd4b6f-9a00-4e4f-9371-f42e06f85907",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "DM9ePwCEC-iq",
        "outputId": "9d202c0b-6457-4761-b428-7508c54ec4c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Out[27]: [4, 16, 36, 64, 100]"
          ]
        }
      ],
      "source": [
        "# Collect the RDD to view its contents\n",
        "even_squared_numbers_rdd.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "e79b141c-0222-4251-9485-a1217114bd7c",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "FQT0pCYxC-iq"
      },
      "source": [
        "### 3. Basic Actions\n",
        "\n",
        "Actions trigger the execution of transformations and return a value to the driver program.\n",
        "\n",
        "#### Reduce\n",
        "\n",
        "The `reduce` action aggregates the elements of the RDD using a specified function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "4eb4d9ad-2fd4-4287-9c8f-3ec35d13cb13",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "y5pW6I6nC-iq",
        "outputId": "ebc68bef-1907-4eb9-9eb6-d5cd4a225c5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Out[28]: 220"
          ]
        }
      ],
      "source": [
        "# Sum all the even squared numbers\n",
        "sum_even_squared_numbers = even_squared_numbers_rdd.reduce(lambda x, y: x + y)\n",
        "sum_even_squared_numbers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "83a36bd7-2ca1-4800-86b0-729e74b9b2fd",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "IK6gDfCEC-iq"
      },
      "source": [
        "#### Collect\n",
        "\n",
        "The `collect` action retrieves the entire RDD to the driver program."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "afd5f2c1-a46f-4eba-92ff-b3c12e18c389",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "1y-5fcXlC-iq",
        "outputId": "f79960e8-6127-47d2-8e8f-2135a4655093"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Out[29]: [4, 16, 36, 64, 100]"
          ]
        }
      ],
      "source": [
        "# Collect the even squared numbers RDD to view its contents\n",
        "even_squared_numbers_rdd.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "bb56f001-911b-41f4-93f2-eae737b92b1c",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "azqeWQufC-iq"
      },
      "source": [
        "#### Count\n",
        "\n",
        "The `count` action returns the number of elements in the RDD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "f742b0ea-a2a3-4744-af0a-7f4dfc5a5a5d",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "H1kwHg6UC-iq",
        "outputId": "4158ffd1-1dff-4d92-cdec-4fe23ea06a5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Out[30]: 5"
          ]
        }
      ],
      "source": [
        "# Count the number of elements in the RDD\n",
        "count_even_squared_numbers = even_squared_numbers_rdd.count()\n",
        "count_even_squared_numbers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "cef2bdc6-3f1e-46fd-8e70-4f71328a7b6e",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "ymfcprvwC-ir"
      },
      "source": [
        "### 4. Working with Text Data\n",
        "\n",
        "#### Reading a Text File\n",
        "\n",
        "You can create an RDD from a text file using the `textFile` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "2eccb033-cde1-488d-bc19-6337fc66c4fb",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "26k9yxTqC-ir"
      },
      "outputs": [],
      "source": [
        "# Read a text file into an RDD\n",
        "text_rdd = sc.textFile(\"sample.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "d8385051-fdc1-4541-8de8-7d2b05c366ab",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "Y7txFwyeC-ir",
        "outputId": "dc85958f-02d6-482c-ea10-afb5fa3e6c8f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
              "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)\n",
              "File \u001b[0;32m<command-1917210368493309>:2\u001b[0m\n",
              "\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Show the first 5 lines of the text file\u001b[39;00m\n",
              "\u001b[0;32m----> 2\u001b[0m \u001b[43mtext_rdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
              "\n",
              "File \u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001b[0m, in \u001b[0;36m_wrap_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
              "\u001b[1;32m     46\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n",
              "\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
              "\u001b[0;32m---> 48\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
              "\u001b[1;32m     49\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog_success(\n",
              "\u001b[1;32m     50\u001b[0m         module_name, class_name, function_name, time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start, signature\n",
              "\u001b[1;32m     51\u001b[0m     )\n",
              "\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
              "\n",
              "File \u001b[0;32m/databricks/spark/python/pyspark/rdd.py:2826\u001b[0m, in \u001b[0;36mRDD.take\u001b[0;34m(self, num)\u001b[0m\n",
              "\u001b[1;32m   2785\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
              "\u001b[1;32m   2786\u001b[0m \u001b[38;5;124;03mTake the first num elements of the RDD.\u001b[39;00m\n",
              "\u001b[1;32m   2787\u001b[0m \n",
              "\u001b[0;32m   (...)\u001b[0m\n",
              "\u001b[1;32m   2823\u001b[0m \u001b[38;5;124;03m[91, 92, 93]\u001b[39;00m\n",
              "\u001b[1;32m   2824\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
              "\u001b[1;32m   2825\u001b[0m items: List[T] \u001b[38;5;241m=\u001b[39m []\n",
              "\u001b[0;32m-> 2826\u001b[0m totalParts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetNumPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
              "\u001b[1;32m   2827\u001b[0m partsScanned \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
              "\u001b[1;32m   2829\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(items) \u001b[38;5;241m<\u001b[39m num \u001b[38;5;129;01mand\u001b[39;00m partsScanned \u001b[38;5;241m<\u001b[39m totalParts:\n",
              "\u001b[1;32m   2830\u001b[0m     \u001b[38;5;66;03m# The number of partitions to try in this iteration.\u001b[39;00m\n",
              "\u001b[1;32m   2831\u001b[0m     \u001b[38;5;66;03m# It is ok for this number to be greater than totalParts because\u001b[39;00m\n",
              "\u001b[1;32m   2832\u001b[0m     \u001b[38;5;66;03m# we actually cap it at totalParts in runJob.\u001b[39;00m\n",
              "\n",
              "File \u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:43\u001b[0m, in \u001b[0;36m_wrap_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
              "\u001b[1;32m     39\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n",
              "\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n",
              "\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(_local, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogging\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m _local\u001b[38;5;241m.\u001b[39mlogging:\n",
              "\u001b[1;32m     42\u001b[0m         \u001b[38;5;66;03m# no need to log since this should be internal call.\u001b[39;00m\n",
              "\u001b[0;32m---> 43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
              "\u001b[1;32m     44\u001b[0m     _local\u001b[38;5;241m.\u001b[39mlogging \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
              "\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
              "\n",
              "File \u001b[0;32m/databricks/spark/python/pyspark/rdd.py:943\u001b[0m, in \u001b[0;36mRDD.getNumPartitions\u001b[0;34m(self)\u001b[0m\n",
              "\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetNumPartitions\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n",
              "\u001b[1;32m    927\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
              "\u001b[1;32m    928\u001b[0m \u001b[38;5;124;03m    Returns the number of partitions in RDD\u001b[39;00m\n",
              "\u001b[1;32m    929\u001b[0m \n",
              "\u001b[0;32m   (...)\u001b[0m\n",
              "\u001b[1;32m    941\u001b[0m \u001b[38;5;124;03m    2\u001b[39;00m\n",
              "\u001b[1;32m    942\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
              "\u001b[0;32m--> 943\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msize()\n",
              "\n",
              "File \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n",
              "\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n",
              "\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n",
              "\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n",
              "\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n",
              "\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n",
              "\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n",
              "\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
              "\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
              "\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
              "\n",
              "File \u001b[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:228\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n",
              "\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n",
              "\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
              "\u001b[0;32m--> 228\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
              "\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
              "\u001b[1;32m    230\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
              "\n",
              "File \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n",
              "\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n",
              "\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n",
              "\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n",
              "\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n",
              "\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n",
              "\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
              "\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n",
              "\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n",
              "\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
              "\n",
              "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1216.partitions.\n",
              ": org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: /sample.txt\n",
              "\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n",
              "\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n",
              "\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n",
              "\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:223)\n",
              "\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:333)\n",
              "\tat scala.Option.getOrElse(Option.scala:189)\n",
              "\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:329)\n",
              "\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:57)\n",
              "\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:333)\n",
              "\tat scala.Option.getOrElse(Option.scala:189)\n",
              "\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:329)\n",
              "\tat org.apache.spark.api.java.JavaRDDLike.partitions(JavaRDDLike.scala:62)\n",
              "\tat org.apache.spark.api.java.JavaRDDLike.partitions$(JavaRDDLike.scala:62)\n",
              "\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\n",
              "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
              "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
              "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
              "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
              "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
              "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n",
              "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
              "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
              "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
              "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n",
              "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n",
              "\tat java.lang.Thread.run(Thread.java:750)\n",
              "Caused by: java.io.IOException: Input path does not exist: /sample.txt\n",
              "\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n",
              "\t... 25 more\n"
            ]
          },
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "arguments": {},
              "data": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)\nFile \u001b[0;32m<command-1917210368493309>:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Show the first 5 lines of the text file\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtext_rdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001b[0m, in \u001b[0;36m_wrap_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 48\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog_success(\n\u001b[1;32m     50\u001b[0m         module_name, class_name, function_name, time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start, signature\n\u001b[1;32m     51\u001b[0m     )\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/rdd.py:2826\u001b[0m, in \u001b[0;36mRDD.take\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   2785\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2786\u001b[0m \u001b[38;5;124;03mTake the first num elements of the RDD.\u001b[39;00m\n\u001b[1;32m   2787\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2823\u001b[0m \u001b[38;5;124;03m[91, 92, 93]\u001b[39;00m\n\u001b[1;32m   2824\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2825\u001b[0m items: List[T] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 2826\u001b[0m totalParts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetNumPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2827\u001b[0m partsScanned \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   2829\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(items) \u001b[38;5;241m<\u001b[39m num \u001b[38;5;129;01mand\u001b[39;00m partsScanned \u001b[38;5;241m<\u001b[39m totalParts:\n\u001b[1;32m   2830\u001b[0m     \u001b[38;5;66;03m# The number of partitions to try in this iteration.\u001b[39;00m\n\u001b[1;32m   2831\u001b[0m     \u001b[38;5;66;03m# It is ok for this number to be greater than totalParts because\u001b[39;00m\n\u001b[1;32m   2832\u001b[0m     \u001b[38;5;66;03m# we actually cap it at totalParts in runJob.\u001b[39;00m\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:43\u001b[0m, in \u001b[0;36m_wrap_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(_local, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogging\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m _local\u001b[38;5;241m.\u001b[39mlogging:\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;66;03m# no need to log since this should be internal call.\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     _local\u001b[38;5;241m.\u001b[39mlogging \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/rdd.py:943\u001b[0m, in \u001b[0;36mRDD.getNumPartitions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetNumPartitions\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m    927\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;124;03m    Returns the number of partitions in RDD\u001b[39;00m\n\u001b[1;32m    929\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;124;03m    2\u001b[39;00m\n\u001b[1;32m    942\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 943\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msize()\n\nFile \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:228\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 228\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    230\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\nFile \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\n\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1216.partitions.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: /sample.txt\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:223)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:333)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:57)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:333)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:329)\n\tat org.apache.spark.api.java.JavaRDDLike.partitions(JavaRDDLike.scala:62)\n\tat org.apache.spark.api.java.JavaRDDLike.partitions$(JavaRDDLike.scala:62)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Input path does not exist: /sample.txt\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t... 25 more\n",
              "errorSummary": "org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: /sample.txt",
              "errorTraceType": "ansi",
              "metadata": {},
              "type": "ipynbError"
            }
          }
        }
      ],
      "source": [
        "# Show the first 5 lines of the text file\n",
        "text_rdd.take(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "2204358b-3ce7-4691-843e-c948cf601bde",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "raeMKq96C-ir"
      },
      "source": [
        "#### FlatMap\n",
        "\n",
        "The `flatMap` transformation splits each line into words, producing a flattened RDD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "534a88d5-e503-4be1-a9bf-c72791032428",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "S3mD5SzBC-ir",
        "outputId": "c6458096-42c3-4b2f-c54a-2dc64a3cc0f4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "arguments": {},
              "data": "",
              "errorSummary": "Command skipped",
              "errorTraceType": "ansi",
              "metadata": {},
              "type": "ipynbError"
            }
          }
        }
      ],
      "source": [
        "# Split each line into words\n",
        "words_rdd = text_rdd.flatMap(lambda line: line.split(\" \"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "c65f5fdc-a576-4cd5-bf32-3c9694e9c80e",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "JwgQd2aoC-ir",
        "outputId": "78f6da4d-1e6a-4d46-9721-f6fed633d96b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "arguments": {},
              "data": "",
              "errorSummary": "Command skipped",
              "errorTraceType": "ansi",
              "metadata": {},
              "type": "ipynbError"
            }
          }
        }
      ],
      "source": [
        "# Collect the RDD to view its contents\n",
        "words_rdd.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "4584ef2e-3743-486e-a6a5-86e308c51710",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "yftGUsm7C-ir"
      },
      "source": [
        "### 5. Key-Value Pair RDDs\n",
        "\n",
        "RDDs can also contain key-value pairs. These RDDs support additional transformations.\n",
        "\n",
        "#### Map to Pair RDD\n",
        "\n",
        "You can create a pair RDD using the `map` transformation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "f312cecf-c308-46be-a5f5-d82607809cc7",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "r_OEhmxfC-ir",
        "outputId": "faaface6-eed0-43b6-bba0-262fc30e7288"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "arguments": {},
              "data": "",
              "errorSummary": "Command skipped",
              "errorTraceType": "ansi",
              "metadata": {},
              "type": "ipynbError"
            }
          }
        }
      ],
      "source": [
        "# Create a pair RDD with each word and the number 1\n",
        "word_pairs_rdd = words_rdd.map(lambda word: (word, 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "e4e8486d-dc34-4dba-8dc1-59faaadc4880",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "2az5cARQC-is",
        "outputId": "8a39a1ed-906b-45d1-b7cb-1f77ed8c27f1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "arguments": {},
              "data": "",
              "errorSummary": "Command skipped",
              "errorTraceType": "ansi",
              "metadata": {},
              "type": "ipynbError"
            }
          }
        }
      ],
      "source": [
        "# Collect the RDD to view its contents\n",
        "word_pairs_rdd.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "050db415-5989-466d-861d-123fec71a541",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "6avINNLqC-is"
      },
      "source": [
        "#### ReduceByKey\n",
        "\n",
        "The `reduceByKey` transformation aggregates values by key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "2a7c25b4-109e-438f-a1ae-7a0c45260b7e",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "OikAYNM1C-is",
        "outputId": "73ce0dd9-34b0-4f8b-d394-1eb573e2e312"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "arguments": {},
              "data": "",
              "errorSummary": "Command skipped",
              "errorTraceType": "ansi",
              "metadata": {},
              "type": "ipynbError"
            }
          }
        }
      ],
      "source": [
        "# Count the occurrences of each word\n",
        "word_counts_rdd = word_pairs_rdd.reduceByKey(lambda a, b: a * b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "ca34f94a-78e7-4df8-858f-053ae9fa1a5f",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "mS8X3UxGC-is",
        "outputId": "8d3fa2d4-d891-4bb5-a8d2-c46965e233b3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "arguments": {},
              "data": "",
              "errorSummary": "Command skipped",
              "errorTraceType": "ansi",
              "metadata": {},
              "type": "ipynbError"
            }
          }
        }
      ],
      "source": [
        "# Collect the RDD to view its contents\n",
        "word_counts_rdd.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "d8575354-6116-4ac4-9170-e3c1b58ab0af",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "yDqq2PC2C-is"
      },
      "source": [
        "### 6. Additional Transformations\n",
        "\n",
        "#### GroupByKey\n",
        "\n",
        "The `groupByKey` transformation groups values with the same key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "9f81c9d4-a357-4e79-a85b-e1749d960a91",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "x6qifgjCC-is",
        "outputId": "d66eb345-ba13-4ca5-86c6-80104a27ebce"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "arguments": {},
              "data": "",
              "errorSummary": "Command skipped",
              "errorTraceType": "ansi",
              "metadata": {},
              "type": "ipynbError"
            }
          }
        }
      ],
      "source": [
        "# Group values by key\n",
        "grouped_words_rdd = word_pairs_rdd.groupByKey().mapValues(list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "38180c32-f131-42c3-9107-4c7502197cbe",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "3fof0NAvC-is",
        "outputId": "5ac1fbf8-3d10-4c16-d49e-e6767d4d4c57"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "arguments": {},
              "data": "",
              "errorSummary": "Command skipped",
              "errorTraceType": "ansi",
              "metadata": {},
              "type": "ipynbError"
            }
          }
        }
      ],
      "source": [
        "# Collect the RDD to view its contents\n",
        "grouped_words_rdd.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "eac70817-e73a-4453-abb0-9b8306f84f2c",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "ZT7EpqZZC-is"
      },
      "source": [
        "#### SortByKey\n",
        "\n",
        "The `sortByKey` transformation sorts the RDD by key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "a34f5dea-4a83-4ba8-9d7e-ebfefd31495d",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "uEzLrep2C-is",
        "outputId": "032aae3d-adb9-4a3f-e961-cf6ee191c98e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "arguments": {},
              "data": "",
              "errorSummary": "Command skipped",
              "errorTraceType": "ansi",
              "metadata": {},
              "type": "ipynbError"
            }
          }
        }
      ],
      "source": [
        "# Sort the word counts RDD by key (word)\n",
        "sorted_word_counts_rdd = word_counts_rdd.sortByKey()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "484bb51a-f15c-4e64-863b-e903973014f0",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "tfrO1PIhC-it",
        "outputId": "8ca30ab0-a3b1-42be-8a9a-266ec53d500c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "arguments": {},
              "data": "",
              "errorSummary": "Command skipped",
              "errorTraceType": "ansi",
              "metadata": {},
              "type": "ipynbError"
            }
          }
        }
      ],
      "source": [
        "# Collect the RDD to view its contents\n",
        "sorted_word_counts_rdd.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "0292d42c-f407-42ce-a2d3-c1f3c45bb104",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "gqFutXmmC-it"
      },
      "source": [
        "### 7. Saving and Loading RDDs\n",
        "\n",
        "#### Save As Text File\n",
        "\n",
        "You can save an RDD to a text file using the `saveAsTextFile` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "8bc470c0-efa7-45f6-adc5-8c2cd72cf3ad",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "44DXzAK4C-it",
        "outputId": "3837ad16-e991-469a-fa78-8531c7ed3155"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "arguments": {},
              "data": "",
              "errorSummary": "Command skipped",
              "errorTraceType": "ansi",
              "metadata": {},
              "type": "ipynbError"
            }
          }
        }
      ],
      "source": [
        "# Save the word counts RDD to a text file\n",
        "word_counts_rdd.saveAsTextFile(\"word_counts\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "998eb42e-08e1-4809-954c-c1a9082864c4",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "5UsKjwv1C-iw"
      },
      "source": [
        "#### Load from Text File\n",
        "\n",
        "You can load the saved RDD from a text file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "7a80386b-e274-452c-98bd-122f6a005be3",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "40BZd13aC-ix",
        "outputId": "93f1511f-de14-4443-a54f-ce52469a7ac7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "arguments": {},
              "data": "",
              "errorSummary": "Command skipped",
              "errorTraceType": "ansi",
              "metadata": {},
              "type": "ipynbError"
            }
          }
        }
      ],
      "source": [
        "# Load the saved RDD from the text file\n",
        "loaded_word_counts_rdd = sc.textFile(\"word_counts\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "14352625-d9aa-452d-a55d-88570ec088dd",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "jB3jbwMfC-ix",
        "outputId": "c6ad86a3-cb76-4fe6-ccee-23029eb333a0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "arguments": {},
              "data": "",
              "errorSummary": "Command skipped",
              "errorTraceType": "ansi",
              "metadata": {},
              "type": "ipynbError"
            }
          }
        }
      ],
      "source": [
        "# Collect the RDD to view its contents\n",
        "loaded_word_counts_rdd.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "63ab800f-044f-4479-9117-65350c376124",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "r6BcVg82C-ix"
      },
      "source": [
        "### Cleanup\n",
        "\n",
        "Always stop the Spark session after completing your tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "90074c31-4374-412f-bc09-2500d837d72b",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "maopDhmdC-ix",
        "outputId": "62f8418a-23aa-434d-fbb2-035d3b77c0e8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "arguments": {},
              "data": "",
              "errorSummary": "Command skipped",
              "errorTraceType": "ansi",
              "metadata": {},
              "type": "ipynbError"
            }
          }
        }
      ],
      "source": [
        "# Stop the Spark session\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "6898b9e9-c9d6-4355-b060-cc4bdfc21a0e",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "0E6q_MgnC-ix"
      },
      "source": [
        "### !Well Done Great Job"
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "computePreferences": null,
      "dashboards": [],
      "environmentMetadata": null,
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 4
      },
      "notebookName": "02-Introduction to RDDs in PySpark",
      "widgets": {}
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
